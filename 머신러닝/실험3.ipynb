{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b98c63dd64cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting preprocessing\n",
      "  Downloading preprocessing-0.1.13-py3-none-any.whl (349 kB)\n",
      "Collecting nltk==3.2.4\n",
      "  Downloading nltk-3.2.4.tar.gz (1.2 MB)\n",
      "Collecting sphinx-rtd-theme==0.2.4\n",
      "  Downloading sphinx_rtd_theme-0.2.4-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: six in c:\\users\\701\\anaconda3\\lib\\site-packages (from nltk==3.2.4->preprocessing) (1.15.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.2.4-py3-none-any.whl size=1367710 sha256=02a447d42b041222d1e1c2990b92f13a454b1f0689feb2e7e22907aa5d2735df\n",
      "  Stored in directory: c:\\users\\701\\appdata\\local\\pip\\cache\\wheels\\d9\\37\\86\\b5270b826e4b542bd6791005300c9d3864059901c7efc03545\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk, sphinx-rtd-theme, preprocessing\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.5\n",
      "    Uninstalling nltk-3.5:\n",
      "      Successfully uninstalled nltk-3.5\n",
      "Successfully installed nltk-3.2.4 preprocessing-0.1.13 sphinx-rtd-theme-0.2.4\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(99)\n",
    "# 인코더의 입력값\n",
    "index_inputs = np.load(open('train_inputs.npy','rb'), allow_pickle=True)\n",
    "# 디코더의 입력값\n",
    "index_outputs = np.load(open('train_outputs.npy','rb'), allow_pickle=True)\n",
    "# 디코더의 타깃값\n",
    "index_targets = np.load(open('train_targets.npy','rb'), allow_pickle=True)\n",
    "# dictonary\n",
    "prepro_configs = json.load(open('data_configs.json'))\n",
    "BATCH_SIZE = 2  # set을 키워보자 -> NoneType 에러가 발생한다. - 메모리이슈\n",
    "MAX_SEQUENCE =25\n",
    "EPOCH =30\n",
    "UNITS =1024\n",
    "EMBEDDING_DIM = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "char2idx = prepro_configs['char2idx']\n",
    "idx2char = prepro_configs['idx2char']\n",
    "std_index = prepro_configs['std_symbol']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                         return_sequences= True,\n",
    "                                         return_state= True,\n",
    "                                         recurrent_initializer= 'glorot_uniform'\n",
    "                                        )\n",
    "    def call(self,x,hidden): \n",
    "        x = self.embedding(x)\n",
    "        output,state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, inp):\n",
    "        return tf.zeros((tf.shape(inp)[0],self.enc_units))\n",
    "class BandanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):  \n",
    "        super(BandanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, values): \n",
    "        hidden_with_time_axis =  tf.expand_dims(query,1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(values)+self.W2(hidden_with_time_axis)\n",
    "                ))\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis =1) \n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units =  dec_units\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences = True,\n",
    "                                        return_state = True,\n",
    "                                        recurrent_initializer = 'glorot_uniform'\n",
    "                                       )\n",
    "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self.attention = BandanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector,attention_weights = self.attention(hidden, enc_output) \n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x], axis =-1)  \n",
    "        output,state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1,output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction= 'none')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n",
    "\n",
    "def loss(real, pred):  \n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0)) \n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    return tf.reduce_mean(acc)\n",
    "class seq2seq(tf.keras.Model):\n",
    "    def __init__(self,vocab_size, embedding_dim, enc_units, dec_units, batch_size, end_token_idx = 2):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_size)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n",
    "        \n",
    "    def call(self,x): \n",
    "        inp, tar = x\n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        predict_tokens  = list()\n",
    "        for t in range(0, tar.shape[1]):\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:,t],1),tf.float32) #특정 state 디코더 입력값\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
    "        result = tf.stack(predict_tokens, axis = 1)\n",
    "        return result\n",
    "    def inference(self, x): \n",
    "        inp = x\n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output,enc_hidden = self.encoder(inp,enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([char2idx[std_index]],1)\n",
    "        predict_tokens = list()\n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            predictions,dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            predict_token = tf.argmax(predictions[0])\n",
    "            if predict_token == self.end_token_idx : \n",
    "                break\n",
    "            predict_tokens.append(predict_token)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token],0),tf.float32)\n",
    "        return tf.stack(predict_tokens, axis =0).numpy()\n",
    "model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS,BATCH_SIZE, char2idx[end_index])\n",
    "model.compile(loss = loss, optimizer= tf.keras.optimizers.Adam(1e-3), metrics =  [accuracy])\n",
    "path = 'data_out/seq2seq_ban'\n",
    "if not(os.path.isdir(path)):\n",
    "    os.makedirs(os.path.join(path))\n",
    "chk_path = path + '/weights.h5'\n",
    "callback = ModelCheckpoint( chk_path, monitor = 'val_accuracy', verbose =1, save_best_only= True,\n",
    "                            save_weights_only =True)\n",
    "earlystop = EarlyStopping(monitor ='val_accuracy', min_delta = 0.001, patience =10)\n",
    "\n",
    "history = model.fit([index_inputs, index_outputs], index_targets,\n",
    "                   batch_size =BATCH_SIZE,\n",
    "                   epochs = EPOCH,\n",
    "                   validation_split= 0.2,\n",
    "                   callbacks = [earlystop, callback])\n",
    "SAVE_FILE_NM = \"weights.h5\"\n",
    "model.load_weights(os.path.join('data_out/seq2seq_ban/weights.h5'))\n",
    "\n",
    "#query = \"뭐야?\"#질문\n",
    "query = input('이이')\n",
    "test_index_inputs , _ = enc_processing([query],char2idx)\n",
    "predict_tokens =  model.inference(test_index_inputs)\n",
    "print(' '.join([idx2char['%s'%t] for  t in predict_tokens]))#대답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
