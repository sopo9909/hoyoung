{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 0s 732us/step - loss: 12.1978 - mse: 12.1978\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 882us/step - loss: 1.3629 - mse: 1.3629\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 905us/step - loss: 0.4535 - mse: 0.4535\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 900us/step - loss: 0.0613 - mse: 0.0613\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 957us/step - loss: 0.0205 - mse: 0.0205\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 698us/step - loss: 0.0111 - mse: 0.0111\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 903us/step - loss: 0.0069 - mse: 0.0069\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 807us/step - loss: 0.0044 - mse: 0.0044  \n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 886us/step - loss: 0.0027 - mse: 0.0027  \n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 696us/step - loss: 0.0017 - mse: 0.0017  \n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 874us/step - loss: 0.0011 - mse: 0.0011  \n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 848us/step - loss: 6.8670e-04 - mse: 6.8670e-04\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 800us/step - loss: 4.3200e-04 - mse: 4.3200e-04\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 699us/step - loss: 2.7169e-04 - mse: 2.7169e-04\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 794us/step - loss: 1.7084e-04 - mse: 1.7084e-04\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 698us/step - loss: 1.0740e-04 - mse: 1.0740e-04\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 768us/step - loss: 6.7514e-05 - mse: 6.7514e-05\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 802us/step - loss: 4.2436e-05 - mse: 4.2436e-05\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 802us/step - loss: 2.6670e-05 - mse: 2.6670e-05\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 783us/step - loss: 1.6760e-05 - mse: 1.6760e-05\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 1.0535e-05 - mse: 1.0535e-05\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 846us/step - loss: 6.6198e-06 - mse: 6.6198e-06\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 697us/step - loss: 4.1587e-06 - mse: 4.1587e-06\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 737us/step - loss: 2.6131e-06 - mse: 2.6131e-06\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 1.6430e-06 - mse: 1.6430e-06\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 806us/step - loss: 1.0320e-06 - mse: 1.0320e-06\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 598us/step - loss: 6.4843e-07 - mse: 6.4843e-07\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 4.0690e-07 - mse: 4.0690e-07\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 905us/step - loss: 2.5606e-07 - mse: 2.5606e-07\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 914us/step - loss: 1.6095e-07 - mse: 1.6095e-07\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 1.0116e-07 - mse: 1.0116e-07\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 949us/step - loss: 6.3504e-08 - mse: 6.3504e-08\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 873us/step - loss: 3.9897e-08 - mse: 3.9897e-08\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 772us/step - loss: 2.5090e-08 - mse: 2.5090e-08\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 736us/step - loss: 1.5690e-08 - mse: 1.5690e-08\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 862us/step - loss: 9.9241e-09 - mse: 9.9241e-09\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 6.2232e-09 - mse: 6.2232e-09\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 777us/step - loss: 3.8859e-09 - mse: 3.8859e-09\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 710us/step - loss: 2.4594e-09 - mse: 2.4594e-09\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 649us/step - loss: 1.5306e-09 - mse: 1.5306e-09\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 741us/step - loss: 9.6736e-10 - mse: 9.6736e-10\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 642us/step - loss: 6.0422e-10 - mse: 6.0422e-10\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 786us/step - loss: 3.8997e-10 - mse: 3.8997e-10\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 830us/step - loss: 2.5417e-10 - mse: 2.5417e-10\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 949us/step - loss: 1.5284e-10 - mse: 1.5284e-10\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 795us/step - loss: 1.0123e-10 - mse: 1.0123e-10\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 813us/step - loss: 6.3377e-11 - mse: 6.3377e-11\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 819us/step - loss: 3.8745e-11 - mse: 3.8745e-11\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 725us/step - loss: 2.6798e-11 - mse: 2.6798e-11\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 863us/step - loss: 1.7958e-11 - mse: 1.7958e-11\n",
      "끝\n",
      "[[ 4.000013 ]\n",
      " [ 4.999999 ]\n",
      " [-3.999999 ]\n",
      " [-3.9999924]]\n",
      "[ 4.  5. -4. -4.]\n",
      "1/1 - 0s - loss: 5.6446e-11 - mse: 5.6446e-11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets,layers ,models\n",
    "from tensorflow.python.keras.backend import conv2d\n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "dataset =np.loadtxt(\"input3_out1.csv\",delimiter=\",\")\n",
    "X = dataset[:,0:3]\n",
    "Y = dataset[:,3]\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y)\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(1, input_dim=3, activation='linear'))\n",
    "model.add(layers.Dense(1))\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd ,loss='mse',metrics=['mse'])\n",
    "model.fit(train_x,train_y,epochs=50,batch_size=1,shuffle=False)\n",
    "print('끝')\n",
    "print(model.predict(test_x)), print(test_y)\n",
    "pred_y = model.predict(test_x)\n",
    "test_loss,test_acc = model.evaluate(test_x,test_y,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도는 :100.0%\n",
      "손실율은 : 0.0%\n",
      "Epoch 1/50\n",
      "576/576 [==============================] - 1s 580us/step - loss: 1.4626 - binary_accuracy: 0.6533\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s 586us/step - loss: 0.6548 - binary_accuracy: 0.6457\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s 558us/step - loss: 0.6524 - binary_accuracy: 0.6457\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s 606us/step - loss: 0.6519 - binary_accuracy: 0.6457\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s 579us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s 639us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s 635us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s 602us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s 614us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s 580us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s 571us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s 545us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s 561us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s 598us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s 577us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 16/50\n",
      "576/576 [==============================] - 0s 566us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 17/50\n",
      "576/576 [==============================] - 0s 662us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 18/50\n",
      "576/576 [==============================] - 0s 597us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 19/50\n",
      "576/576 [==============================] - 0s 668us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 20/50\n",
      "576/576 [==============================] - 0s 694us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 21/50\n",
      "576/576 [==============================] - 0s 588us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 22/50\n",
      "576/576 [==============================] - 0s 656us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 23/50\n",
      "576/576 [==============================] - 0s 549us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 24/50\n",
      "576/576 [==============================] - 0s 593us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 25/50\n",
      "576/576 [==============================] - 0s 576us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 26/50\n",
      "576/576 [==============================] - 0s 592us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 27/50\n",
      "576/576 [==============================] - 0s 572us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 28/50\n",
      "576/576 [==============================] - 0s 624us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 29/50\n",
      "576/576 [==============================] - 0s 549us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 30/50\n",
      "576/576 [==============================] - 0s 558us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 31/50\n",
      "576/576 [==============================] - 0s 702us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 32/50\n",
      "576/576 [==============================] - 0s 667us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 33/50\n",
      "576/576 [==============================] - 0s 568us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 34/50\n",
      "576/576 [==============================] - 0s 550us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 35/50\n",
      "576/576 [==============================] - 0s 534us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 36/50\n",
      "576/576 [==============================] - 0s 522us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 37/50\n",
      "576/576 [==============================] - 0s 526us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 38/50\n",
      "576/576 [==============================] - 0s 522us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 39/50\n",
      "576/576 [==============================] - 0s 519us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 40/50\n",
      "576/576 [==============================] - 0s 670us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 41/50\n",
      "576/576 [==============================] - 0s 543us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 42/50\n",
      "576/576 [==============================] - 0s 552us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 43/50\n",
      "576/576 [==============================] - 0s 525us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 44/50\n",
      "576/576 [==============================] - 0s 539us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 45/50\n",
      "576/576 [==============================] - 0s 536us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 46/50\n",
      "576/576 [==============================] - 0s 517us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 47/50\n",
      "576/576 [==============================] - 0s 523us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 48/50\n",
      "576/576 [==============================] - 0s 515us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 49/50\n",
      "576/576 [==============================] - 0s 573us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "Epoch 50/50\n",
      "576/576 [==============================] - 0s 562us/step - loss: 0.6518 - binary_accuracy: 0.6457\n",
      "[[0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]\n",
      " [0.37980545]]\n",
      "[1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 - 0s - loss: 0.6131 - binary_accuracy: 0.7240\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "dataset =np.loadtxt(\"input3_out1.csv\",delimiter=\",\")\n",
    "X = dataset[:,0:3]\n",
    "Y = dataset[:,3]\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(train_x,train_y)\n",
    "pred_y = model.predict(test_x)\n",
    "def rmse(y_real,y_pred):\n",
    "    return np.sqrt(np.mean((y_real-y_pred)**2))\n",
    "np.round(rmse(test_y,pred_y),2)\n",
    "aa=np.round(np.sqrt(mean_squared_error(test_y,pred_y)),2)\n",
    "print(\"정확도는 :{}%\".format(model.score(test_x, test_y)*100))\n",
    "print('손실율은 : {}%'.format(aa))\n",
    "\n",
    "#로지스특\n",
    "dataset =np.loadtxt(\"pima-indians-diabetes.csv\",delimiter=\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y)\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(8,activation='relu'))\n",
    "model.add(Dense(1, input_dim=1, activation='sigmoid'))\n",
    "sgd=optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd ,loss='binary_crossentropy',metrics=['binary_accuracy'])\n",
    "model.fit(train_x,train_y, batch_size=1, epochs=50, shuffle=False)\n",
    "print(model.predict(test_x)), print(test_y)\n",
    "pred_y = model.predict(test_x)\n",
    "test_loss,test_acc = model.evaluate(test_x,test_y,verbose=2)\n",
    "#print(model.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 1.1436 - accuracy: 0.6000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1371 - accuracy: 0.6000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1306 - accuracy: 0.6000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1242 - accuracy: 0.6000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1178 - accuracy: 0.6000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1115 - accuracy: 0.6000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1052 - accuracy: 0.6000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0989 - accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0927 - accuracy: 0.6000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0866 - accuracy: 0.6000\n",
      "값을 넣어주세요15\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028B711D89D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x = [2,  4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "y = [0,  0, 0, 0,  0,   0,  1,   1,   1,  1]\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(2,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x,y,epochs=5)\n",
    "#result=model.evaluate(x,y)\n",
    "x2 = int(input('값을 넣어주세요'))\n",
    "newdata = np.array([x2])\n",
    "if model.predict(np.reshape(newdata,(1,1)))[0][0] >=1:\n",
    "    print(1)\n",
    "else :\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "576/576 [==============================] - 1s 595us/step - loss: 2.4759 - binary_accuracy: 0.6814\n",
      "Epoch 2/50\n",
      "576/576 [==============================] - 0s 617us/step - loss: 0.6327 - binary_accuracy: 0.6853\n",
      "Epoch 3/50\n",
      "576/576 [==============================] - 0s 632us/step - loss: 0.6292 - binary_accuracy: 0.6846\n",
      "Epoch 4/50\n",
      "576/576 [==============================] - 0s 663us/step - loss: 0.6282 - binary_accuracy: 0.6847\n",
      "Epoch 5/50\n",
      "576/576 [==============================] - 0s 607us/step - loss: 0.6280 - binary_accuracy: 0.6847\n",
      "Epoch 6/50\n",
      "576/576 [==============================] - 0s 623us/step - loss: 0.6279 - binary_accuracy: 0.6847\n",
      "Epoch 7/50\n",
      "576/576 [==============================] - 0s 642us/step - loss: 0.6279 - binary_accuracy: 0.6847\n",
      "Epoch 8/50\n",
      "576/576 [==============================] - 0s 597us/step - loss: 0.6278 - binary_accuracy: 0.6847\n",
      "Epoch 9/50\n",
      "576/576 [==============================] - 0s 592us/step - loss: 0.6279 - binary_accuracy: 0.6837\n",
      "Epoch 10/50\n",
      "576/576 [==============================] - 0s 605us/step - loss: 0.6276 - binary_accuracy: 0.6838\n",
      "Epoch 11/50\n",
      "576/576 [==============================] - 0s 614us/step - loss: 0.6278 - binary_accuracy: 0.6837\n",
      "Epoch 12/50\n",
      "576/576 [==============================] - 0s 645us/step - loss: 0.6277 - binary_accuracy: 0.6837\n",
      "Epoch 13/50\n",
      "576/576 [==============================] - 0s 623us/step - loss: 0.6277 - binary_accuracy: 0.6838\n",
      "Epoch 14/50\n",
      "576/576 [==============================] - 0s 617us/step - loss: 0.6278 - binary_accuracy: 0.6837\n",
      "Epoch 15/50\n",
      "576/576 [==============================] - 0s 625us/step - loss: 0.6277 - binary_accuracy: 0.6837\n",
      "Epoch 16/50\n",
      "576/576 [==============================] - 0s 619us/step - loss: 0.6277 - binary_accuracy: 0.6837\n",
      "Epoch 17/50\n",
      "576/576 [==============================] - 0s 645us/step - loss: 0.6277 - binary_accuracy: 0.6837\n",
      "Epoch 18/50\n",
      "576/576 [==============================] - 0s 750us/step - loss: 0.6277 - binary_accuracy: 0.6837\n",
      "Epoch 19/50\n",
      "576/576 [==============================] - 0s 642us/step - loss: 0.6277 - binary_accuracy: 0.6837\n",
      "Epoch 20/50\n",
      "576/576 [==============================] - 0s 605us/step - loss: 0.6277 - binary_accuracy: 0.6847\n",
      "Epoch 21/50\n",
      "576/576 [==============================] - 0s 598us/step - loss: 0.6277 - binary_accuracy: 0.6847\n",
      "Epoch 22/50\n",
      "576/576 [==============================] - 0s 594us/step - loss: 0.6277 - binary_accuracy: 0.6847\n",
      "Epoch 23/50\n",
      "576/576 [==============================] - 0s 674us/step - loss: 0.6277 - binary_accuracy: 0.6847\n",
      "Epoch 24/50\n",
      "576/576 [==============================] - 0s 658us/step - loss: 0.6277 - binary_accuracy: 0.6847\n",
      "Epoch 25/50\n",
      "576/576 [==============================] - 0s 603us/step - loss: 0.6277 - binary_accuracy: 0.6847\n",
      "Epoch 26/50\n",
      "576/576 [==============================] - 0s 598us/step - loss: 0.6277 - binary_accuracy: 0.6847\n",
      "Epoch 27/50\n",
      "576/576 [==============================] - 0s 649us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 28/50\n",
      "576/576 [==============================] - 0s 638us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 29/50\n",
      "576/576 [==============================] - 0s 639us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 30/50\n",
      "576/576 [==============================] - 0s 664us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 31/50\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 32/50\n",
      "576/576 [==============================] - 0s 652us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 33/50\n",
      "576/576 [==============================] - 0s 596us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 34/50\n",
      "576/576 [==============================] - 0s 610us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 35/50\n",
      "576/576 [==============================] - 0s 597us/step - loss: 0.6276 - binary_accuracy: 0.6847\n",
      "Epoch 36/50\n",
      "576/576 [==============================] - 0s 603us/step - loss: 0.6275 - binary_accuracy: 0.6847\n",
      "Epoch 37/50\n",
      "576/576 [==============================] - 0s 588us/step - loss: 0.6275 - binary_accuracy: 0.6847\n",
      "Epoch 38/50\n",
      "576/576 [==============================] - 0s 647us/step - loss: 0.6275 - binary_accuracy: 0.6847\n",
      "Epoch 39/50\n",
      "576/576 [==============================] - 0s 685us/step - loss: 0.6275 - binary_accuracy: 0.6847\n",
      "Epoch 40/50\n",
      "576/576 [==============================] - 0s 679us/step - loss: 0.6274 - binary_accuracy: 0.6847\n",
      "Epoch 41/50\n",
      "576/576 [==============================] - 0s 664us/step - loss: 0.6274 - binary_accuracy: 0.6847\n",
      "Epoch 42/50\n",
      "576/576 [==============================] - 0s 676us/step - loss: 0.6268 - binary_accuracy: 0.6854\n",
      "Epoch 43/50\n",
      "576/576 [==============================] - 0s 641us/step - loss: 0.6288 - binary_accuracy: 0.6837\n",
      "Epoch 44/50\n",
      "576/576 [==============================] - 0s 635us/step - loss: 0.6274 - binary_accuracy: 0.6846\n",
      "Epoch 45/50\n",
      "576/576 [==============================] - 0s 677us/step - loss: 0.6275 - binary_accuracy: 0.6846\n",
      "Epoch 46/50\n",
      "576/576 [==============================] - 0s 636us/step - loss: 0.6276 - binary_accuracy: 0.6846\n",
      "Epoch 47/50\n",
      "576/576 [==============================] - 0s 610us/step - loss: 0.6276 - binary_accuracy: 0.6846\n",
      "Epoch 48/50\n",
      "576/576 [==============================] - 0s 670us/step - loss: 0.6276 - binary_accuracy: 0.6846\n",
      "Epoch 49/50\n",
      "576/576 [==============================] - 0s 628us/step - loss: 0.6276 - binary_accuracy: 0.6846\n",
      "Epoch 50/50\n",
      "576/576 [==============================] - 0s 595us/step - loss: 0.6276 - binary_accuracy: 0.6846\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028B6F8DD1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]\n",
      " [0.36194825]]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#로지스특\n",
    "dataset =np.loadtxt(\"pima-indians-diabetes.csv\",delimiter=\",\")\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y)\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(8,activation='relu'))\n",
    "model.add(Dense(1, input_dim=1, activation='sigmoid'))\n",
    "sgd=optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd ,loss='binary_crossentropy',metrics=['binary_accuracy'])\n",
    "model.fit(train_x,train_y, batch_size=1, epochs=50, shuffle=False)\n",
    "print(model.predict(test_x)), print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 69.3381 - mse: 69.3381\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 784us/step - loss: 20.4212 - mse: 20.4212\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 1.1007 - mse: 1.1007\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0118 - mse: 0.0118\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 698us/step - loss: 0.0058 - mse: 0.0058\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0017 - mse: 0.0017\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 9.1284e-04 - mse: 9.1284e-04\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 692us/step - loss: 4.8798e-04 - mse: 4.8798e-04\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 888us/step - loss: 2.6052e-04 - mse: 2.6052e-04\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 970us/step - loss: 1.3896e-04 - mse: 1.3896e-04\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 7.4060e-05 - mse: 7.4060e-05\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 864us/step - loss: 3.9464e-05 - mse: 3.9464e-05\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 2.1025e-05 - mse: 2.1025e-05\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 1.1196e-05 - mse: 1.1196e-05\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 804us/step - loss: 5.9598e-06 - mse: 5.9598e-06\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 3.1715e-06 - mse: 3.1715e-06\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 867us/step - loss: 1.6896e-06 - mse: 1.6896e-06\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 8.9832e-07 - mse: 8.9832e-07\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 883us/step - loss: 4.7962e-07 - mse: 4.7962e-07\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 2.5483e-07 - mse: 2.5483e-07\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 987us/step - loss: 1.3588e-07 - mse: 1.3588e-07\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 910us/step - loss: 7.2289e-08 - mse: 7.2289e-08\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 3.8513e-08 - mse: 3.8513e-08\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 2.0482e-08 - mse: 2.0482e-08\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 1.0929e-08 - mse: 1.0929e-08\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 883us/step - loss: 5.7978e-09 - mse: 5.7978e-09\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 3.0780e-09 - mse: 3.0780e-09\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 1.5916e-09 - mse: 1.5916e-09\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 8.1767e-10 - mse: 8.1767e-10\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 4.8974e-10 - mse: 4.8974e-10\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 2.3014e-10 - mse: 2.3014e-10\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 887us/step - loss: 1.2519e-10 - mse: 1.2519e-10\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 7.7724e-11 - mse: 7.7724e-11\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 2.2210e-11 - mse: 2.2210e-11\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 897us/step - loss: 2.2473e-11 - mse: 2.2473e-11\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 822us/step - loss: 5.5021e-12 - mse: 5.5021e-12\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 911us/step - loss: 9.8891e-12 - mse: 9.8891e-12\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 8.7769e-12 - mse: 8.7769e-12\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 1.0617e-12 - mse: 1.0617e-12\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 9.5755e-13 - mse: 9.5755e-13\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 770us/step - loss: 1.1952e-12 - mse: 1.1952e-12\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 4.8395e-13 - mse: 4.8395e-13\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 2.1892e-13 - mse: 2.1892e-13\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 656us/step - loss: 5.2288e-13 - mse: 5.2288e-13\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 698us/step - loss: 1.7239e-12 - mse: 1.7239e-12\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 1.2302e-12 - mse: 1.2302e-12\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 5.3101e-13 - mse: 5.3101e-13\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 849us/step - loss: 9.9480e-14 - mse: 9.9480e-14\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 757us/step - loss: 4.4475e-13 - mse: 4.4475e-13\n",
      "끝\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028B71077790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[3.0000002]\n",
      " [5.       ]\n",
      " [3.9999998]\n",
      " [5.0000005]]\n",
      "[3. 5. 4. 5.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "dataset =np.loadtxt(\"input3_out1.csv\",delimiter=\",\")\n",
    "X = dataset[:,0:3]\n",
    "Y = dataset[:,3]\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y)\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(1, input_dim=3, activation='linear'))\n",
    "model.add(layers.Dense(1))\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd ,loss='mse',metrics=['mse'])\n",
    "model.fit(train_x,train_y,epochs=50,batch_size=1,shuffle=False)\n",
    "print('끝')\n",
    "print(model.predict(test_x)), print(test_y)\n",
    "\n",
    "#loss,acc = model.evaluate(test_x, test_y, verbose=2)\n",
    "#print(loss,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
