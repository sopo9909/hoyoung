{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "206/206 [==============================] - 143s 659ms/step - loss: 1.4794 - accuracy: 0.0142\n",
      "Epoch 2/80\n",
      "206/206 [==============================] - 135s 657ms/step - loss: 1.1790 - accuracy: 0.0447\n",
      "Epoch 3/80\n",
      "206/206 [==============================] - 135s 657ms/step - loss: 0.9880 - accuracy: 0.0473\n",
      "Epoch 4/80\n",
      "206/206 [==============================] - 135s 657ms/step - loss: 0.8926 - accuracy: 0.0517\n",
      "Epoch 5/80\n",
      "206/206 [==============================] - 135s 656ms/step - loss: 0.8219 - accuracy: 0.0568\n",
      "Epoch 6/80\n",
      "206/206 [==============================] - 135s 657ms/step - loss: 0.7433 - accuracy: 0.0629\n",
      "Epoch 7/80\n",
      "206/206 [==============================] - 136s 659ms/step - loss: 0.6754 - accuracy: 0.0706\n",
      "Epoch 8/80\n",
      "206/206 [==============================] - 135s 657ms/step - loss: 0.5931 - accuracy: 0.0795\n",
      "Epoch 9/80\n",
      "206/206 [==============================] - 135s 657ms/step - loss: 0.5112 - accuracy: 0.0883\n",
      "Epoch 10/80\n",
      "206/206 [==============================] - 135s 658ms/step - loss: 0.4303 - accuracy: 0.0982\n",
      "Epoch 11/80\n",
      "206/206 [==============================] - 135s 658ms/step - loss: 0.3513 - accuracy: 0.1083\n",
      "Epoch 12/80\n",
      "206/206 [==============================] - 135s 658ms/step - loss: 0.2715 - accuracy: 0.1188\n",
      "Epoch 13/80\n",
      "206/206 [==============================] - 136s 658ms/step - loss: 0.2080 - accuracy: 0.1303\n",
      "Epoch 14/80\n",
      "206/206 [==============================] - 138s 671ms/step - loss: 0.1534 - accuracy: 0.1382\n",
      "Epoch 15/80\n",
      "206/206 [==============================] - 159s 770ms/step - loss: 0.1103 - accuracy: 0.1467\n",
      "Epoch 16/80\n",
      "206/206 [==============================] - 164s 795ms/step - loss: 0.0810 - accuracy: 0.1518\n",
      "Epoch 17/80\n",
      "206/206 [==============================] - 164s 795ms/step - loss: 0.0639 - accuracy: 0.1553\n",
      "Epoch 18/80\n",
      "206/206 [==============================] - 164s 795ms/step - loss: 0.0546 - accuracy: 0.1567\n",
      "Epoch 19/80\n",
      "206/206 [==============================] - 164s 795ms/step - loss: 0.0497 - accuracy: 0.1578\n",
      "Epoch 20/80\n",
      "206/206 [==============================] - 141s 685ms/step - loss: 0.0467 - accuracy: 0.1587\n",
      "Epoch 21/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0422 - accuracy: 0.1582\n",
      "Epoch 22/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0364 - accuracy: 0.1598\n",
      "Epoch 23/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0333 - accuracy: 0.1610\n",
      "Epoch 24/80\n",
      "206/206 [==============================] - 137s 667ms/step - loss: 0.0298 - accuracy: 0.1614\n",
      "Epoch 25/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0274 - accuracy: 0.1625\n",
      "Epoch 26/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0257 - accuracy: 0.1622\n",
      "Epoch 27/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0237 - accuracy: 0.1637\n",
      "Epoch 28/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0221 - accuracy: 0.1638\n",
      "Epoch 29/80\n",
      "206/206 [==============================] - 138s 667ms/step - loss: 0.0200 - accuracy: 0.1637\n",
      "Epoch 30/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0198 - accuracy: 0.1641\n",
      "Epoch 31/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0184 - accuracy: 0.1642\n",
      "Epoch 32/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0180 - accuracy: 0.1648\n",
      "Epoch 33/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0173 - accuracy: 0.1637\n",
      "Epoch 34/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0168 - accuracy: 0.1650\n",
      "Epoch 35/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0158 - accuracy: 0.1630\n",
      "Epoch 36/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0146 - accuracy: 0.1646\n",
      "Epoch 37/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0142 - accuracy: 0.1647\n",
      "Epoch 38/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0144 - accuracy: 0.1647\n",
      "Epoch 39/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0136 - accuracy: 0.1661\n",
      "Epoch 40/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0133 - accuracy: 0.1654\n",
      "Epoch 41/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0124 - accuracy: 0.1657\n",
      "Epoch 42/80\n",
      "206/206 [==============================] - 138s 668ms/step - loss: 0.0120 - accuracy: 0.1661\n",
      "Epoch 43/80\n",
      "206/206 [==============================] - 138s 669ms/step - loss: 0.0120 - accuracy: 0.1657\n",
      "Epoch 44/80\n",
      "206/206 [==============================] - 138s 670ms/step - loss: 0.0116 - accuracy: 0.1651\n",
      "Epoch 45/80\n",
      "206/206 [==============================] - 140s 681ms/step - loss: 0.0118 - accuracy: 0.1661\n",
      "Epoch 46/80\n",
      "206/206 [==============================] - 139s 675ms/step - loss: 0.0106 - accuracy: 0.1657\n",
      "Epoch 47/80\n",
      "206/206 [==============================] - 139s 675ms/step - loss: 0.0115 - accuracy: 0.1668\n",
      "Epoch 48/80\n",
      "206/206 [==============================] - 138s 671ms/step - loss: 0.0108 - accuracy: 0.1657\n",
      "Epoch 49/80\n",
      "206/206 [==============================] - 139s 673ms/step - loss: 0.0106 - accuracy: 0.1658\n",
      "Epoch 50/80\n",
      "206/206 [==============================] - 139s 673ms/step - loss: 0.0101 - accuracy: 0.1664\n",
      "Epoch 51/80\n",
      "206/206 [==============================] - 139s 673ms/step - loss: 0.0105 - accuracy: 0.1658\n",
      "Epoch 52/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0102 - accuracy: 0.1655\n",
      "Epoch 53/80\n",
      "206/206 [==============================] - 139s 673ms/step - loss: 0.0096 - accuracy: 0.1670\n",
      "Epoch 54/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0097 - accuracy: 0.1666\n",
      "Epoch 55/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0097 - accuracy: 0.1663\n",
      "Epoch 56/80\n",
      "206/206 [==============================] - 139s 673ms/step - loss: 0.0092 - accuracy: 0.1656\n",
      "Epoch 57/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0092 - accuracy: 0.1662\n",
      "Epoch 58/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0093 - accuracy: 0.1659\n",
      "Epoch 59/80\n",
      "206/206 [==============================] - 139s 675ms/step - loss: 0.0094 - accuracy: 0.1664\n",
      "Epoch 60/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0093 - accuracy: 0.1669\n",
      "Epoch 61/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0089 - accuracy: 0.1663\n",
      "Epoch 62/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0086 - accuracy: 0.1659\n",
      "Epoch 63/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0084 - accuracy: 0.1660\n",
      "Epoch 64/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0087 - accuracy: 0.1663\n",
      "Epoch 65/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0087 - accuracy: 0.1666\n",
      "Epoch 66/80\n",
      "206/206 [==============================] - 139s 674ms/step - loss: 0.0084 - accuracy: 0.1675\n",
      "Epoch 67/80\n",
      "206/206 [==============================] - 163s 793ms/step - loss: 0.0090 - accuracy: 0.1668\n",
      "Epoch 68/80\n",
      "206/206 [==============================] - 169s 821ms/step - loss: 0.0085 - accuracy: 0.1662\n",
      "Epoch 69/80\n",
      "206/206 [==============================] - 159s 772ms/step - loss: 0.0084 - accuracy: 0.1664\n",
      "Epoch 70/80\n",
      "206/206 [==============================] - 161s 782ms/step - loss: 0.0081 - accuracy: 0.1673\n",
      "Epoch 71/80\n",
      "206/206 [==============================] - 161s 783ms/step - loss: 0.0084 - accuracy: 0.1663\n",
      "Epoch 72/80\n",
      "206/206 [==============================] - 151s 732ms/step - loss: 0.0080 - accuracy: 0.1667\n",
      "Epoch 73/80\n",
      "206/206 [==============================] - 152s 740ms/step - loss: 0.0084 - accuracy: 0.1664\n",
      "Epoch 74/80\n",
      "206/206 [==============================] - 152s 739ms/step - loss: 0.0081 - accuracy: 0.1671\n",
      "Epoch 75/80\n",
      "206/206 [==============================] - 152s 737ms/step - loss: 0.0077 - accuracy: 0.1671\n",
      "Epoch 76/80\n",
      "206/206 [==============================] - 153s 743ms/step - loss: 0.0078 - accuracy: 0.1656\n",
      "Epoch 77/80\n",
      "206/206 [==============================] - 171s 830ms/step - loss: 0.0079 - accuracy: 0.1658\n",
      "Epoch 78/80\n",
      "206/206 [==============================] - 156s 759ms/step - loss: 0.0078 - accuracy: 0.1660\n",
      "Epoch 79/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 141s 686ms/step - loss: 0.0079 - accuracy: 0.1668\n",
      "Epoch 80/80\n",
      "206/206 [==============================] - 169s 819ms/step - loss: 0.0081 - accuracy: 0.1664\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import random\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "from flask import Flask, request, make_response\n",
    "from slacker import Slacker\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from preprocessing import *\n",
    "import selenium\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pyperclip\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "  def get_config(self):\n",
    "\tconfig = super().get_config().copy()\n",
    "\treturn config\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "    position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "    i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "    d_model=d_model)\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "    angle_rads = np.zeros(angle_rads.shape)\n",
    "    angle_rads[:, 0::2] = sines\n",
    "    angle_rads[:, 1::2] = cosines\n",
    "    pos_encoding = tf.constant(angle_rads)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output, attention_weights\n",
    "np.set_printoptions(suppress=True)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    assert d_model % self.num_heads == 0\n",
    "    self.depth = d_model // self.num_heads\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "  def get_config(self):\n",
    "\tconfig = super().get_config().copy()\n",
    "\treturn config\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "    concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))\n",
    "    outputs = self.dense(concat_attention)\n",
    "    return outputs  \n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': padding_mask \n",
    "      })\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "  return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "def encoder(vocab_size, num_layers, dff,d_model, num_heads, dropout,name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name=\"encoder_layer_{}\".format(i),)([outputs, padding_mask])\n",
    "  return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  attention1 = MultiHeadAttention(\n",
    "    d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "      'query': inputs, 'key': inputs, 'value': inputs,\n",
    "      'mask': look_ahead_mask \n",
    "      })\n",
    "  attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "          'mask': padding_mask # 패딩 마스크\n",
    "      })\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "def decoder(vocab_size, num_layers, dff,d_model, num_heads, dropout,name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "def transformer(vocab_size, num_layers, dff,d_model, num_heads, dropout,name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask, output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask]) \n",
    "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "MAX_LENGTH=40\n",
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n",
    "  #loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "  return tf.reduce_mean(loss)\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "  def get_config(self):\n",
    "\tconfig = super().get_config().copy()\n",
    "\treturn config\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "#sample_learning_rate = CustomSchedule(d_model=128)\n",
    "#urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\", filename=\"ChatBotData.csv\")\n",
    "####데이터\n",
    "#####\n",
    "####\n",
    "train_data = pd.read_csv('total_data.csv')\n",
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  questions.append(sentence)\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  answers.append(sentence)\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "sample_string = questions[20]\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "  {'inputs': questions,'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
    "  },\n",
    "  {\n",
    "  'outputs': answers[:, 1:]  \n",
    "  },))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "tf.keras.backend.clear_session()\n",
    "# Hyper-parameters\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "model = transformer(vocab_size=VOCAB_SIZE,num_layers=NUM_LAYERS,dff=DFF,d_model=D_MODEL,num_heads=NUM_HEADS,dropout=DROPOUT)\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy]) #loss_function ,'sparse_categorical_crossentropy'\n",
    "model.fit(dataset, epochs=80)\n",
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "  sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "  return tf.squeeze(output, axis=0)\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "  predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "  return predicted_sentence\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Layer PositionalEncoding has arguments in `__init__` and therefore must override `get_config`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a07e01569ddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_chat.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \"\"\"\n\u001b[0;32m   2000\u001b[0m     \u001b[1;31m# pylint: enable=line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2001\u001b[1;33m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[0;32m   2002\u001b[0m                     signatures, options, save_traces)\n\u001b[0;32m   2003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    151\u001b[0m           \u001b[1;34m'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m--> 153\u001b[1;33m     hdf5_format.save_model_to_hdf5(\n\u001b[0m\u001b[0;32m    154\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[0;32m    155\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0mmodel_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[1;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[0;32m    156\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m   metadata = dict(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[1;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[0;32m    153\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[0mmodel_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mget_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[1;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m     \u001b[0mlayer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mserialize_layer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inbound_nodes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[1;34m(instance)\u001b[0m\n\u001b[0;32m    248\u001b[0m         return serialize_keras_class_and_config(\n\u001b[0;32m    249\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[1;34m(instance)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m       \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mget_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[1;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m     \u001b[0mlayer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mserialize_layer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inbound_nodes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[1;34m(instance)\u001b[0m\n\u001b[0;32m    248\u001b[0m         return serialize_keras_class_and_config(\n\u001b[0;32m    249\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[1;32m--> 250\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[1;34m(instance)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m       \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mget_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[1;31m# or that `get_config` has been overridden:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_is_default'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m       raise NotImplementedError('Layer %s has arguments in `__init__` and '\n\u001b[0m\u001b[0;32m    700\u001b[0m                                 \u001b[1;34m'therefore must override `get_config`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                                 self.__class__.__name__)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Layer PositionalEncoding has arguments in `__init__` and therefore must override `get_config`."
     ]
    }
   ],
   "source": [
    "model.save(\"model_chat.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import random\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "from flask import Flask, request, make_response\n",
    "from slacker import Slacker\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from preprocessing import *\n",
    "import selenium\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model =load_model(\"model_chat.h5\")\n",
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "  sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "  return tf.squeeze(output, axis=0)\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "  predicted_sentence = tokenizer.decode([i for i in prediction if i < tokenizer.vocab_size])\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "  return predicted_sentence\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"xoxb-1545617412550-1661382617969-nfSQ5Q6ZXlg8uGPn5TrOQm9B\" #Bot\n",
    "slack = Slacker(token)\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://search.naver.com/search.naver?query=날씨')\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "data1 = soup.find('div', {'class':'weather_box'})\n",
    "find_address = data1.find('span', {'class':'btn_select'}).text\n",
    "#print('현재 위치: '+find_address)\n",
    "find_currenttemp = data1.find('span',{'class': 'todaytemp'}).text\n",
    "data2 = soup.find('div', {'class':'weather_box'})\n",
    "tommow_morning = data2.find('div', {'class':'tomorrow_area _mainTabContent'})\n",
    "find_tommowtemp = tommow_morning.find('p', {'class':'cast_txt'}).text\n",
    "tommow_afternoon = data2.find_all('p', {'class':'cast_txt'})\n",
    "a=[]\n",
    "menu=['고기국수','쌀국수','동파육','탕수육','비빔밥','리조또','피자','맥도날드','버거킹','칼국수','비빔국수','막국수','잔치국수','냉면','콩국수','비빔냄면','짜장면','짬뽕','불고기','돼지갈비','닭갈비','떡갈비','삼겹살','뒷고기','제육볶음','두루치기','닭강정','닭볶음탕','닭발','양념치킨','간장치킨','보쌈','편육','백숙','육회','옻닭','찜닭','순대','간장게장','매운탕','아귀찜','해물찜','회덮밥','꼼장어볶음','부침개','파전','갈비탕','닭곰탕','도가니탕','내장탕','알탕','추어탕','매운탕','파스타','삼계탕','부대찌개','초밥','라멘','라면','샌드위치','순두부찌개','된장찌개','떡볶이','감자탕','카레','쭈꾸미볶음']\n",
    "for i in tommow_afternoon:\n",
    "    a.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###안 해도 됨\n",
    "\n",
    "def clipboard_input(user_xpath, user_input):\n",
    "    temp_user_input = pyperclip.paste()  # 사용자 클립보드를 따로 저장\n",
    "    pyperclip.copy(user_input)\n",
    "    driver.find_element_by_xpath(user_xpath).click()\n",
    "    ActionChains(driver).key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).perform()\n",
    "    pyperclip.copy(temp_user_input)  # 사용자 클립보드에 저장 된 내용을 다시 가져 옴\n",
    "    time.sleep(1)\n",
    "###############################################################################################################################\n",
    "def start():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('window-size=1920,1080')\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "#options.add_argument('headless')\n",
    "    driver = webdriver.Chrome(executable_path='chromedriver', options=options)\n",
    "    URL = 'https://vibe.naver.com/today'\n",
    "    driver.get(url=URL)\n",
    "    driver.implicitly_wait(10)\n",
    "    login = {\n",
    "    \"id\" : \"sopo9909\",\n",
    "    \"pw\" : \"hhy3242\"\n",
    "    }\n",
    "    driver.find_element_by_xpath('//*[@id=\"app\"]/div[2]/div/div/a[2]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"header\"]/div[2]/div[1]/a/span').click()\n",
    "    driver.implicitly_wait(2)\n",
    "    clipboard_input('//*[@id=\"id\"]', login.get(\"id\"))\n",
    "    clipboard_input('//*[@id=\"pw\"]', login.get(\"pw\"))\n",
    "    driver.find_element_by_xpath('//*[@id=\"log.login\"]').click()\n",
    "    driver.find_element_by_xpath('//*[@id=\"header\"]/a[1]').click()\n",
    "def search_music(send_music):\n",
    "    search = driver.find_element_by_xpath('//*[@id=\"search_keyword\"]')\n",
    "    search.send_keys(send_music)\n",
    "    search.send_keys(Keys.ENTER)\n",
    "    driver.implicitly_wait(2)\n",
    "    driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div/div/div[1]/a').click()\n",
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [29/Jan/2021 17:48:38] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "[2021-01-29 17:49:13,686] ERROR in app: Exception on / [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-4-474d4e353879>\", line 138, in hello_there\n",
      "    return event_handler(event_type, slack_event)\n",
      "  File \"<ipython-input-4-474d4e353879>\", line 124, in event_handler\n",
      "    slack.chat.post_message(channel, predict(user_query))\n",
      "NameError: name 'predict' is not defined\n",
      "127.0.0.1 - - [29/Jan/2021 17:49:13] \"\u001b[35m\u001b[1mPOST / HTTP/1.1\u001b[0m\" 500 -\n",
      "127.0.0.1 - - [29/Jan/2021 17:49:29] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Jan/2021 17:49:45] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Jan/2021 17:49:56] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Jan/2021 17:50:04] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Jan/2021 17:50:12] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "[2021-01-29 17:54:13,722] ERROR in app: Exception on / [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\701\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-4-474d4e353879>\", line 138, in hello_there\n",
      "    return event_handler(event_type, slack_event)\n",
      "  File \"<ipython-input-4-474d4e353879>\", line 124, in event_handler\n",
      "    slack.chat.post_message(channel, predict(user_query))\n",
      "NameError: name 'predict' is not defined\n",
      "127.0.0.1 - - [29/Jan/2021 17:54:13] \"\u001b[35m\u001b[1mPOST / HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "def event_handler(event_type, slack_event):\n",
    "    channel = slack_event[\"event\"][\"channel\"]\n",
    "    string_slack_event = str(slack_event)\n",
    "    if string_slack_event.find(\"{'type': 'user', 'user_id': \") != -1:  # 멘션으로 호출\n",
    "        try:\n",
    "            user_query = slack_event['event']['blocks'][0]['elements'][0]['elements'][1]['text']\n",
    "            so = \"강다영은 어때\"\n",
    "            so1 = \"황호영은 어떤\"\n",
    "            so11 = \"이한비는 어때\"\n",
    "            so12 = \"성희님\"\n",
    "            so13 = \"태익님\"\n",
    "            so14 = \"현수님\"\n",
    "            so15 = \"은이님\"\n",
    "            so2 = \"김광희는 어때\"\n",
    "            so3 = \"너는 누구 좋아해\"\n",
    "            so4 = \"만들었어?\"\n",
    "            so5 = '내일 날씨'\n",
    "            so6 = '오늘 날씨'\n",
    "            so7 = '뭐먹지'\n",
    "            so8 = '뭐 먹지'\n",
    "            so9 = '틀어줘'\n",
    "            so10 ='코로나 확진자'\n",
    "            global k \n",
    "            #user_query1 =user_query.split(\" \")\n",
    "            if so in user_query:    \n",
    "                #answer = get_answer(user_query)\n",
    "                slack.chat.post_message(channel, \"강다영은 조금...바보예요\")\n",
    "            elif so1 in user_query:\n",
    "                slack.chat.post_message(channel, \"황호영은 정말 열심히 하는 사람이예요.\")\n",
    "            elif so11 in user_query:\n",
    "                slack.chat.post_message(channel, \"우리는 그녀를 빛이라고 부르죠.\")\n",
    "            elif so12 in user_query:\n",
    "                slack.chat.post_message(channel, \"그분이 저를 보고싶어하신다는 분 아닌가요?! 근데..어디 가신 거죠..??\")\n",
    "            elif so13 in user_query:\n",
    "                slack.chat.post_message(channel, \"알죠알죠! 저는 멕시코 형이라고 기억하고 있어요\")\n",
    "            elif so14 in user_query:\n",
    "                slack.chat.post_message(channel, \"그분 요즘 다이어트에 흠뻑 빠졌어요\")\n",
    "            elif so15 in user_query:\n",
    "                slack.chat.post_message(channel, \"우리 친해질까요...??\")\n",
    "            elif so2 in user_query:\n",
    "                slack.chat.post_message(channel, \"김광희는 착한 것 같아요.\")\n",
    "            elif so3 in user_query:\n",
    "                slack.chat.post_message(channel, \"전 멋진 호영님을 좋아합니다.\")\n",
    "            elif so4 in user_query:\n",
    "                slack.chat.post_message(channel, \"대단한 호영님께서 만들어주셨어요\")\n",
    "            elif so5 in user_query:\n",
    "                slack.chat.post_message(channel, '내일의 오전은 '+a[1]+', 오후는 '+a[2]+\"입니다.\")\n",
    "            elif so6 in user_query:\n",
    "                slack.chat.post_message(channel, a[0]+'. '+find_address+'의 현재 온도는 '+find_currenttemp+\"도 입니다.\")\n",
    "            elif so7 in user_query:\n",
    "                x = random.randint(0, 66)\n",
    "                slack.chat.post_message(channel, '오늘은 '+menu[x]+\" 괜찮으세요?\")\n",
    "            elif so8 in user_query:\n",
    "                y = random.randint(0, 66)\n",
    "                slack.chat.post_message(channel, '오늘 메뉴는 '+menu[y]+\" 어떠세요?\")\n",
    "            elif so9 in user_query:\n",
    "                if k ==0:\n",
    "                    send_music=user_query.replace('틀어줘','')\n",
    "                #if '노래' in send_music:\n",
    "                #    send_music=send_music.replace('노래','')\n",
    "                    slack.chat.post_message(channel, send_music+' 재생하겠습니다')\n",
    "                    k +=1\n",
    "                    options = webdriver.ChromeOptions()\n",
    "                    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "    #options.add_argument('window-size=1920,1080')\n",
    "#options.add_argument('headless')\n",
    "                    driver = webdriver.Chrome(executable_path='chromedriver', options=options)\n",
    "                    URL = 'https://vibe.naver.com/today'\n",
    "                    driver.get(url=URL)\n",
    "                    driver.implicitly_wait(10)\n",
    "                    login = {\n",
    "                    \"id\" : \"sopo9909\",\n",
    "                    \"pw\" : \"hhy3242\"\n",
    "                    }\n",
    "                    driver.find_element_by_xpath('//*[@id=\"app\"]/div[2]/div/div/a[2]').click()\n",
    "                    time.sleep(2)\n",
    "                    driver.implicitly_wait(2)\n",
    "                    driver.find_element_by_xpath('//*[@id=\"header\"]/div[2]/div[1]/a/span').click()\n",
    "                    driver.implicitly_wait(2)\n",
    "                    time.sleep(1)\n",
    "                    driver.find_element_by_xpath('//*[@id=\"id\"]').click()\n",
    "                    #def clipboard_input(user_xpath, user_input):\n",
    "                    temp_user_input = pyperclip.paste()  # 사용자 클립보드를 따로 저장\n",
    "                    pyperclip.copy(login.get(\"id\"))\n",
    "                    driver.find_element_by_xpath('//*[@id=\"id\"]').click()\n",
    "                    ActionChains(driver).key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).perform()\n",
    "                    pyperclip.copy(temp_user_input)  # 사용자 클립보드에 저장 된 내용을 다시 가져 옴\n",
    "                    time.sleep(1)\n",
    "                    #clipboard_input('//*[@id=\"id\"]', login.get(\"id\"))\n",
    "                    driver.implicitly_wait(1)\n",
    "                    temp_user_input = pyperclip.paste()  # 사용자 클립보드를 따로 저장\n",
    "                    pyperclip.copy(login.get(\"pw\"))\n",
    "                    driver.find_element_by_xpath('//*[@id=\"pw\"]').click()\n",
    "                    ActionChains(driver).key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).perform()\n",
    "                    pyperclip.copy(temp_user_input)  # 사용자 클립보드에 저장 된 내용을 다시 가져 옴\n",
    "                    time.sleep(1)\n",
    "                    driver.find_element_by_xpath('//*[@id=\"pw\"]').click()\n",
    "                    #clipboard_input('//*[@id=\"pw\"]', login.get(\"pw\"))\n",
    "                    driver.implicitly_wait(1)\n",
    "                    driver.find_element_by_xpath('//*[@id=\"log.login\"]').click()\n",
    "                    driver.implicitly_wait(3)\n",
    "                    driver.find_element_by_xpath('//*[@id=\"header\"]/a[1]').click()\n",
    "                    driver.implicitly_wait(3)\n",
    "                    search = driver.find_element_by_xpath('//*[@id=\"search_keyword\"]')\n",
    "                    search.send_keys(send_music)\n",
    "                    search.send_keys(Keys.ENTER)\n",
    "                    driver.implicitly_wait(2)\n",
    "                    driver.find_element_by_xpath('//*[@id=\"content\"]/div[2]/div/div/div[1]/a').click()\n",
    "                    #play_music(send_music)\n",
    "                else:\n",
    "                    pass\n",
    "            elif so10 in user_query:\n",
    "                html = requests.get('https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=%EC%BD%94%EB%A1%9C%EB%82%98',headers={'User-Agent':'Mozilla/5.0'})\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "                data1 = soup.find('div', {'class':'status_info'})\n",
    "                    #print(data1)\n",
    "                total_corona = data1.find('p', {'class':'info_num'}).text\n",
    "                slack.chat.post_message(channel, '한국의 코로나 총 확진자 수는 '+total_corona+\"명 입니다.\")\n",
    "                    #print(total_corona)\n",
    "                corona = data1.find('em', {'class':'info_variation'}).text\n",
    "                slack.chat.post_message(channel, '오늘 확진자 수는 '+corona+\"명 입니다.\")\n",
    "            else:\n",
    "                slack.chat.post_message(channel, predict(user_query))\n",
    "            #slack.chat.post_message(channel, \"아직 그것은 학습하지 못 하였어요\")\n",
    "            return make_response(\"ok\", 200, )\n",
    "        except IndexError:\n",
    "            pass\n",
    "    message = \"[%s] cannot find event handler\" % event_type\n",
    "    return make_response(message, 200, {\"X-Slack-No-Retry\": 1})\n",
    "@app.route('/', methods=['POST'])\n",
    "def hello_there():\n",
    "    slack_event = json.loads(request.data)\n",
    "    if \"challenge\" in slack_event:\n",
    "        return make_response(slack_event[\"challenge\"], 200, {\"content_type\": \"application/json\"}) \n",
    "    if \"event\" in slack_event:\n",
    "        event_type = slack_event[\"event\"][\"type\"]\n",
    "        return event_handler(event_type, slack_event)\n",
    "    return make_response(\"There are no slack request events\", 404, {\"X-Slack-No-Retry\": 1})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run('0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
