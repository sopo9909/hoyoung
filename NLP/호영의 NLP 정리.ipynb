{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 호영의 꼬꼬마 및 NLP 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리(NLP)\n",
    "* 언어모델\n",
    "    - 순차적 언어모델(시퀀셜 단어 모델)(기계적 단어 모델)\n",
    "        * 단어1(나는)0.5 * 단어2(너를)0.1 * [단어3(좋아해)0.2]\n",
    "        * 단어추천 시스템예 (자동 완성)\n",
    "        * 수집된 단어를 분해-> 단어의 순서대로 가중치 모델 설정 -> 알고리즘으로 추천\n",
    "        \n",
    "    - 통계적 언어모델\n",
    "        * 전체 단어의 개수 카운트 기반의 모델\n",
    "        * 조건부 확률 정리기반 (베이지안 정리) 문장 기반의 처리\n",
    "        \n",
    "    - 인공지능 기반의 언어모델\n",
    "        * ML \n",
    "        * 딥러닝 기반\n",
    "        \n",
    "    - 융합형 모델\n",
    "        * 융합형 데이터 처리가 가능\n",
    "* 각 모델별 한계\n",
    "    - 순차기반 언어: 언어 순서의 자유도\n",
    "        * 나는 너를 매우 사랑해 / 너를 나는 매우 사랑해 / 매우 너를 나는 사랑해\n",
    "        * i love you so much/ you love i so much / love i you much so\n",
    "    - 띄어 쓰기 문제\n",
    "        * 나는너를사랑해\n",
    "        * iloveyousomuch\n",
    "    - 데이터 풀의 문제\n",
    "        * 꿹 너를 P(성북구 너를)/P(꿹:0) 분모가 0이 되는 경우 조건부 확률 사용 불가\n",
    "        * 충분한 데이터가 없을경우 사용 불가: 데이터 희소성 문제(data sparsity problem)\n",
    "\n",
    "        * ? 단어 블럭으로 알아보면 되지 않을까?\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반 모델 BOW\n",
    "    - BOW(Bag of Words)\n",
    "        * 순서기반보다 유사 의미 파악 가능\n",
    "        * 유사 문장 기반의 의미 분석\n",
    "|문장|나는| 너를| 매우| 사랑해|모두|\n",
    "|--------------------|--|--|--|--|--|\n",
    "|*나는 너를 매우 사랑해|1| 1| 1| 1|0|\n",
    "|*너를 나는 매우 사랑해|1| 1| 1| 1|0|\n",
    "|*매우 너를 나는 사랑해|1| 1| 1| 1|0|\n",
    "|나는 너를 사랑해|1| 1| 0| 1|0|\n",
    "|나는 매우 모두 사랑해|1| 1| 0| 1|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반모델\n",
    "    - N-gram\n",
    "        * 언어의 순서 기반의 의미 부여\n",
    "        * 전체 문단의 맥락을 부여 가능\n",
    "        \n",
    "    - 예\n",
    "        * 나는 너를 매우 사랑해 진짜 / 너를 나는 진짜 매우 사랑해 \n",
    "    - 2 gram ( bi gram)\n",
    "        * 나는 너를\n",
    "        * 너를 매우\n",
    "        * 매우 사랑해\n",
    "        * 사랑해 진짜\n",
    "    - 3 gram ( tri gram)\n",
    "        * 나는 너를 매우\n",
    "        * 너를 매우 사랑해\n",
    "* n-gram 기반 BOW\n",
    "    * 장점\n",
    "        - 문장간 미묘한 의미 분석\n",
    "        - 주제의 강도 파악\n",
    "        - 전체적 주제파악이 디테일\n",
    "    * 단점\n",
    "        - 희소의 문제 발생: 빅데이터가 필요\n",
    "        - n 숫자 지정의 문제: 하이퍼 파라미터 조정-> 펄플렉서티\n",
    "            * 펄플렉서티: 모호함 계수 -> 낮을수록\n",
    "        - Term Document Matrix(DTM,TDM)\n",
    "        - 한 줄은 BOW\n",
    "    \n",
    "|문장|나는 너를| 너를 매우| 매우 사랑해| 너를 나는|나는 매우|\n",
    "|--------------------|--|--|--|--|--|\n",
    "|*나는 너를 매우 사랑해|1| 1| 1| 0|0|\n",
    "|*너를 나는 매우 사랑해|0| 0| 1| 1|1|\n",
    "|*매우 너를 나는 사랑해|0| 0|0| 1|0|\n",
    "|나는 너를 사랑해|1| 0| 0| 0|0|\n",
    "|나는 매우 모두 사랑해|0| 0| 0| 0|0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주제 ?\n",
    "* 많이 나오는 단어?\n",
    "    - 의미있는 단어(명사,형용사,동사)\n",
    "    - 조사, 어미 등은 가중치\n",
    "* 다른 기사에서도 많이 나오는 단어는 늘 쓰는 말\n",
    "    - 형식적인 말들\n",
    "    - 늘 쓰는 단어들은 중요하지 않다\n",
    "* TF(Term of Frequency) 단어 빈도 / IDF 역빈도(Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP( 자연어 처리 )\n",
    "* BOW (Bag of words)\n",
    "    - Word Count\n",
    "        - 단어수 기반의 맵을 만들어서 의미 분석\n",
    "* 단어를 수집하여 문장의 특성을 파악\n",
    "- 아버지가방에들어가신다\n",
    "- 형태소분석 > 의미 분석\n",
    "    - 형태소 분석기: konlpy\n",
    "        - mecab\n",
    "        - kkma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP 단계\n",
    "* nlp 관련 엔진의 기능에 연관\n",
    "\n",
    "*  문서 전처리\n",
    "    - Document(문서/전자문서):여러개 있음\n",
    "    - corpus(말뭉치):모음/스크래이핑 \n",
    "    - Tokenize(의미있는 단위로 분리)\n",
    "        - 단어장 사전처리: 인명, 신조어, 분야별 단어\n",
    "        - 문장,단어,띄어쓰기,형태소 morpheme 분석\n",
    "        - 언어에서 의미를 가지는 최소단위- 형태소의 명칭:품사POS part of speech]\n",
    "            * 명사 동사 형용사 부사 ....\n",
    "        - 한국형 형태소 분석기 KONLPY\n",
    "            * JAVA 기반의 환경조성(자바설치,JPYPE)\n",
    "        - 영미권 단어 쪼개기\n",
    "            * 표제어 추출(Lemmatization): 형태소 분석- 사전형 단어분석 -> 품사정리\n",
    "            * 어간추출(stemming) un- in- 일종의 형태소 분석\n",
    "                - 단어의 의미를 담고 있는 단어의 핵심 부분.\n",
    "    - 불용어 처리 : stopwords\n",
    "* 자연어 처리 기법\n",
    "    - 카운트기반 자연어 처리\n",
    "        - BOW Bag of words\n",
    "        - TDM\n",
    "        - TF-IDF\n",
    "    \n",
    "\n",
    "* 목적하는 언어셋 수집 (코퍼스corpus:말뭉치) > 문장(sentence) > 단어(words) > 형태소 분석\n",
    "\n",
    "* 말뭉치 수집(Web Crawling 돌아다니기/Web Scraping 정보가져오기 ) \n",
    "    - 말뭉치 또는 코퍼스(Corpus)는 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합이\n",
    "* 문장 토큰화(Sentence Tokenization)\n",
    "    - 문장 토큰화: 문장의 마침표(.), 개행문자(\\n), 느낌표(!), 물음표(?) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적.\n",
    "    \n",
    "* 단어(품사) 토큰화(Word Tokenization)\n",
    "    - konlpy 형태소 분석 등을 이용한 단어 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 융합형 데이터 분석\n",
    "    * 이종데이터 결합을 통한 분석\n",
    "## 의미분석\n",
    "    * 잠재의미분석(LSA:  Latent Semantic Analysis)\n",
    "        - 기존의 TDM 및 TF-IDF 가 단어의 의미 무시하고 숫자에 의한 처리\n",
    "        * SVD( 특이값 분해 ) Truncated\n",
    "            - m*n*l의 데이터-> m*m    m*n   n*n\n",
    "        * PCA( 주성분 분석)\n",
    "            - 데이터의 중심(평균)->0\n",
    "            - 중심으로부터 각 데이터 벡터에 대한 공분산 행렬터 각 데이터 벡터에 대한 공분산 행렬\n",
    "            - 고유벡터 기반의 차원축소\n",
    "    * 잠재 디리클레 할당(LDA:Latent Dirichlet Allocation)\n",
    "        - 디리클레 분포 활용\n",
    "        - 문서의 토픽 발견 프로세스\n",
    "    * 차이\n",
    "        - LSA 단순 차원 축소를 통한 단어 빈도 기반의 추청\n",
    "        - LDA 단어가 특정 주제에 존재할 확률 과 특정 주제가 존재할 확률의 결합 확률 \n",
    "        \n",
    "    * https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집\n",
    "    * 스크래이핑을 통한 수집\n",
    "        - 뉴스\n",
    "        - SNS\n",
    "        - OTT\n",
    "        - 댓글\n",
    "    * json api를 활용\n",
    "        - 핀테크 금융\n",
    "        - 프롭테크 부동산\n",
    "        - 마케팅\n",
    "        - SCM\n",
    "        - CRM\n",
    "    * 빅데이터 생성\n",
    "    * DB 활용\n",
    "    * 시각화\n",
    "    * 사무 자동화\n",
    "        - 오피스\n",
    "        - RPA\n",
    "    * 사회소통능력 \n",
    "        - 유튜브\n",
    "        - SNS\n",
    "        - 홈페이지 관리\n",
    "## 개별 데이터 분석\n",
    "    * 정형 데이터 분석: 개별분석\n",
    "        - 모멘텀 분석\n",
    "        - 기초 분석\n",
    "    * 비정형 데이터 분석: 개별분석\n",
    "        - Count Vectorize\n",
    "        - TF-IDF\n",
    "        - Word2Vec:cbow\n",
    "        - LDA\n",
    "\n",
    "## 데이터 융합\n",
    "    * 정형데이터: 주가 데이터\n",
    "    * 비정형 데이터: 뉴스 데이터\n",
    "    * 데이터 융합\n",
    "        - 데이터베이스 융합: DB/SQL join\n",
    "        - Pandas 활용: Merge 명령어 사용\n",
    "## 고급 데이터 처리 기법\n",
    "*  해당분야 용어 기법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSub():\n",
    "    dbname='navernews.db' #디비명\n",
    "    with sqlite3.connect(dbname) as conn:\n",
    "        cursor=conn.cursor()\n",
    "        sql='select sarticle from subnews '#불러옥;\n",
    "        res=cursor.execute(sql).fetchall()\n",
    "        result=[]\n",
    "        for r in res:\n",
    "            result.append(r[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KKMA\n",
    "    * POS는 Part Of Speech로 형태소 품사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma=Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictsort(r):\n",
    "    sr=sorted(r.items(),key=(lambda x:x[1]),reverse=True)\n",
    "    return sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    NNG : 일반명사\n",
    "    NNP : 고유명사\n",
    "    NNB : 의존명사\n",
    "    NNM : 단위 의존 명사\n",
    "    VV : 동사\n",
    "    VA : 형용사\n",
    "    VXV : 보조동사\n",
    "    VXA : 보조형용사\n",
    "    VCP : 긍정 지정사, ~이다\n",
    "    VCN : 부정 지정사, ~아니다\n",
    "    SF : 마침표, 물음표, 느낌표\n",
    "    EFN : 평서형 종결어미\n",
    "    EFQ : 의문형 종결어미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPOS(rres): \n",
    "    p=kkma.pos(rres)\n",
    "    dictpos={}\n",
    "    words=[]\n",
    "    #표제어 추출\n",
    "    check=['NNG','NNP','NNB','NNM','VV','VA','VXV','VXA','VCP','VCN','SF','EFN','EFQ']\n",
    "    for  w,pos in p:\n",
    "        if(pos in check):\n",
    "            words.append(w)\n",
    "            if (w in dictpos.keys()):\n",
    "                dictpos[w]+=1\n",
    "            else:\n",
    "                dictpos[w]=1\n",
    "    return (dictpos,words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorpus(doc):\n",
    "    getp=getPOS(doc)\n",
    "    sen=' '.join(getp[1])\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for doc in docs:\n",
    "    getp=getCorpus(doc)\n",
    "    corpus.append(getp)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반 TDM\n",
    "    - min_df document frequency 문서내 발견 횟수\n",
    "    - 사이킷런의 CountVectorizer를 통해 피처 생성\n",
    "    - 정규표현식을 사용해 토큰을 추출한다.\n",
    "    - 모두 소문자로 변환시키기 때문에 good, Good, gOod이 모두 같은 특성이 된다.\n",
    "    - 의미 없는 특성을 많이 생성하기 때문에 적어도 두 개의 문서에 나타난 토큰만을 사용한다.\n",
    "    - min_df로 토큰이 나타날 최소 문서 개수를 지정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -DictVectorizer: 각 단어의 수를 세어놓은 사전에서 BOW 인코딩 벡터를 만든다.\n",
    "    -CountVectorizer:문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩 벡터를 만든다.\n",
    "    -TfidfVectorizer:CountVectorizer와 비슷하지만 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 인코딩 벡터를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countvectorizer\n",
    "    *stop_words : 문자열 {‘english’}, 리스트 또는 None (디폴트)\n",
    "        - vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "        - vect.vocabulary_\n",
    "    *stop words 목록.‘english’이면 영어용 스탑 워드 사용.\n",
    "    *analyzer : 문자열 {‘word’, ‘char’, ‘char_wb’} 또는 함수 ,단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램\n",
    "    *token_pattern : string, 토큰 정의용 정규 표현식\n",
    "    *tokenizer : 함수 또는 None (디폴트), 토큰 생성 함수 .\n",
    "    *ngram_range : (min_n, max_n) 튜플, n-그램 범위\n",
    "    *max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1, 단어장에 포함되기 위한 최대 빈도\n",
    "    *min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 디폴트 1 ,단어장에 포함되기 위한 최소 빈도\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(min_df=6,ngram_range=(1,3))\n",
    "tdm=vect.fit_transform(corpus).toarray()\n",
    "tdm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words=list(vect.vocabulary_.keys())# 전체 단어 갯수 파악용\n",
    "words=list(vect.get_feature_names())\n",
    "#print(words)\n",
    "i=0\n",
    "for td in tdm:\n",
    "    zt=list(zip(words,td))\n",
    "    szt=sorted(zt,key=(lambda x:x[1]),reverse=True)\n",
    "    #print(corpus[i])\n",
    "    #print(szt[:5])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 방식으로 중요단어 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법이다.\n",
    "\n",
    "* 구제적으로는 문서 d(document)와 단어 t 에 대해 다음과 같이 계산한다.\n",
    "\n",
    "* tf-idf(d,t)=tf(d,t)⋅idf(t)\n",
    "\n",
    "* tf(d,t): term frequency. 특정한 단어의 빈도수\n",
    "\n",
    "* idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    "\n",
    "* idf(d,t)=logn1+df(t)\n",
    "* n : 전체 문서의 수\n",
    "\n",
    "* df(t): 단어 t를 가진 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv=TfidfVectorizer(min_df=6,ngram_range=(1,3))\n",
    "tfm=tfidfv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주제 분석(모델링) Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svdm=TruncatedSVD()\n",
    "svdm.fit(tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components=svdm.components_\n",
    "np.shape(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca=list(tfidfv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDict(components,voca):\n",
    "    res=[]\n",
    "    for c in components:\n",
    "        dict={}\n",
    "        for i,v in enumerate(c):\n",
    "            dict[voca[i]]=v\n",
    "        res.append(dict)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=makeDict(components,voca)\n",
    "for r in res:\n",
    "    print(\"*\"*50)\n",
    "    sr=sorted(r.items(),key=(lambda x:x[1]),reverse=True)\n",
    "    print(sr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda=LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(tfm)\n",
    "components=lda.components_\n",
    "voca=list(tfidfv.get_feature_names())\n",
    "print(np.shape(components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=makeDict(components,voca)\n",
    "for r in res:\n",
    "    print(\"*\"*50)\n",
    "    sr=sorted(r.items(),key=(lambda x:x[1]),reverse=True)\n",
    "    print(sr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:2]\n",
    "corpus_w=[]\n",
    "for c in corpus:\n",
    "    cs=c.split(' ')\n",
    "    corpus_w.append(cs)\n",
    "corpus_w[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "cor_dict=corpora.Dictionary(corpus_w)\n",
    "cor_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_m=[]\n",
    "for cor in corpus_w:\n",
    "    res=cor_dict.doc2bow(cor)\n",
    "    corpus_m.append(res)\n",
    "corpus_m[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "N_TOPIC=10\n",
    "N_PASS=10\n",
    "ldagen=gensim.models.ldamodel.LdaModel(corpus_m,num_topics=N_TOPIC,id2word=cor_dict,passes=N_PASS) \n",
    "topics=ldagen.print_topics()\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis=pyLDAvis.gensim_models.prepare(ldagen,corpus_m,cor_dict)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적의 토픽 갯수는?\n",
    "* Perplexity\n",
    "    - 혼란한 정도 : 낮을 수로 좋다\n",
    "    - 모델간 비교에서 상대적 우위\n",
    "    - 수치적 한계 -> 실제 해석상 무리가 있을 수도 있음\n",
    "    \n",
    "* Coherence\n",
    "    - 주제의 일관성 : 높을수록 좋다\n",
    "    - 주제를 이루는 단어들의 유사성으로 파악\n",
    "    - 수치적 한계\n",
    "    - 상대적 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상대적 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "N_PASS=10\n",
    "x=[]\n",
    "ycoh=[]\n",
    "yperp=[]\n",
    "for i in range(2,20):\n",
    "    N_TOPIC=i\n",
    "    ldagen=gensim.models.ldamodel.LdaModel(corpus_m,num_topics=N_TOPIC,id2word=cor_dict,passes=N_PASS) \n",
    "    cm=CoherenceModel(model=ldagen,corpus=corpus_m,coherence='u_mass')\n",
    "    coh=cm.get_coherence()\n",
    "    perp=ldagen.log_perplexity(corpus_m)\n",
    "    x.append(i)\n",
    "    ycoh.append(coh)\n",
    "    yperp.append(perp)\n",
    "    print('coherence:',coh,'perpleixty:',perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "N_TOPIC=2\n",
    "x=[]\n",
    "ycoh=[]\n",
    "yperp=[]\n",
    "for i in range(2,20):\n",
    "    N_PASS=i\n",
    "    ldagen=gensim.models.ldamodel.LdaModel(corpus_m,num_topics=N_TOPIC,id2word=cor_dict,passes=N_PASS) \n",
    "    cm=CoherenceModel(model=ldagen,corpus=corpus_m,coherence='u_mass')\n",
    "    coh=cm.get_coherence()\n",
    "    perp=ldagen.log_perplexity(corpus_m)\n",
    "    x.append(i)\n",
    "    ycoh.append(coh)\n",
    "    yperp.append(perp)\n",
    "    print('coherence:',coh,'perpleixty:',perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PASS=10\n",
    "N_TOPIC=2\n",
    "ldagen=gensim.models.ldamodel.LdaModel(corpus_m,num_topics=N_TOPIC,id2word=cor_dict,passes=N_PASS) \n",
    "topics=ldagen.print_topics()\n",
    "vis=pyLDAvis.gensim_models.prepare(ldagen,corpus_m,cor_dict)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
