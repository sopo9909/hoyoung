{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4729/4729 [==============================] - 11864s 3s/step - loss: 1.2879 - accuracy: 0.8565 - val_loss: 1.5315 - val_accuracy: 0.8595\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85949, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 2/30\n",
      "4729/4729 [==============================] - 9752s 2s/step - loss: 0.8987 - accuracy: 0.8604 - val_loss: 1.6252 - val_accuracy: 0.8665\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.85949 to 0.86645, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 3/30\n",
      "4729/4729 [==============================] - 9724s 2s/step - loss: 0.5941 - accuracy: 0.8686 - val_loss: 1.7820 - val_accuracy: 0.8755\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.86645 to 0.87554, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 4/30\n",
      "4729/4729 [==============================] - 9714s 2s/step - loss: 0.3525 - accuracy: 0.8781 - val_loss: 1.9394 - val_accuracy: 0.8854\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.87554 to 0.88539, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 5/30\n",
      "4729/4729 [==============================] - 9775s 2s/step - loss: 0.1716 - accuracy: 0.8879 - val_loss: 2.1075 - val_accuracy: 0.8956\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.88539 to 0.89559, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 6/30\n",
      "4729/4729 [==============================] - 9997s 2s/step - loss: 0.0737 - accuracy: 0.8978 - val_loss: 2.1982 - val_accuracy: 0.9049\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.89559 to 0.90492, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 7/30\n",
      "4729/4729 [==============================] - 11447s 2s/step - loss: 0.0436 - accuracy: 0.9065 - val_loss: 2.2638 - val_accuracy: 0.9125\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.90492 to 0.91251, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 8/30\n",
      "4729/4729 [==============================] - 10779s 2s/step - loss: 0.0344 - accuracy: 0.9136 - val_loss: 2.2658 - val_accuracy: 0.9185\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.91251 to 0.91849, saving model to data_out/seq2seq_ban\\weights.h5\n",
      "Epoch 9/30\n",
      "4399/4729 [==========================>...] - ETA: 11:16 - loss: 0.0268 - accuracy: 0.9192"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-74e4df5691c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m history = model.fit([index_inputs, index_outputs], index_targets,\n\u001b[0m\u001b[0;32m    156\u001b[0m                    \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "tf.random.set_seed(99)\n",
    "index_inputs = np.load(open('train_inputs.npy','rb'), allow_pickle=True)\n",
    "index_outputs = np.load(open('train_outputs.npy','rb'), allow_pickle=True)\n",
    "index_targets = np.load(open('train_targets.npy','rb'), allow_pickle=True)\n",
    "prepro_configs = json.load(open('data_configs.json'))\n",
    "BATCH_SIZE = 2 \n",
    "MAX_SEQUENCE =25\n",
    "EPOCH =30\n",
    "UNITS =1024\n",
    "EMBEDDING_DIM = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "char2idx = prepro_configs['char2idx']\n",
    "idx2char = prepro_configs['idx2char']\n",
    "std_index = prepro_configs['std_symbol']\n",
    "end_index = prepro_configs['end_symbol']\n",
    "vocab_size = prepro_configs['vocab_size']\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                         return_sequences= True,\n",
    "                                         return_state= True,\n",
    "                                         recurrent_initializer= 'glorot_uniform'\n",
    "                                        )\n",
    "    def call(self,x,hidden): \n",
    "        x = self.embedding(x)\n",
    "        output,state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, inp):\n",
    "        return tf.zeros((tf.shape(inp)[0],self.enc_units))\n",
    "class BandanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):  \n",
    "        super(BandanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, values): \n",
    "        hidden_with_time_axis =  tf.expand_dims(query,1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(values)+self.W2(hidden_with_time_axis)\n",
    "                ))\n",
    "        attention_weights = tf.nn.softmax(score,axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis =1) \n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units =  dec_units\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences = True,\n",
    "                                        return_state = True,\n",
    "                                        recurrent_initializer = 'glorot_uniform'\n",
    "                                       )\n",
    "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self.attention = BandanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector,attention_weights = self.attention(hidden, enc_output) \n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector,1),x], axis =-1)  \n",
    "        output,state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1,output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction= 'none')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n",
    "\n",
    "def loss(real, pred):  \n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0)) \n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    return tf.reduce_mean(acc)\n",
    "class seq2seq(tf.keras.Model):\n",
    "    def __init__(self,vocab_size, embedding_dim, enc_units, dec_units, batch_size, end_token_idx = 2):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.end_token_idx = end_token_idx\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_size)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n",
    "    def call(self,x): \n",
    "        inp, tar = x\n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        predict_tokens  = list()\n",
    "        for t in range(0, tar.shape[1]):\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:,t],1),tf.float32) #특정 state 디코더 입력값\n",
    "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
    "        result = tf.stack(predict_tokens, axis = 1)\n",
    "        return result\n",
    "    def inference(self, x): \n",
    "        inp = x\n",
    "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
    "        enc_output,enc_hidden = self.encoder(inp,enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([char2idx[std_index]],1)\n",
    "        predict_tokens = list()\n",
    "        for t in range(0, MAX_SEQUENCE):\n",
    "            predictions,dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
    "            predict_token = tf.argmax(predictions[0])\n",
    "            if predict_token == self.end_token_idx : \n",
    "                break\n",
    "            predict_tokens.append(predict_token)\n",
    "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token],0),tf.float32)\n",
    "        return tf.stack(predict_tokens, axis =0).numpy()\n",
    "model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS,BATCH_SIZE, char2idx[end_index])\n",
    "model.compile(loss = loss, optimizer= tf.keras.optimizers.Adam(1e-3), metrics =  [accuracy])\n",
    "path = 'data_out/seq2seq_ban'\n",
    "if not(os.path.isdir(path)):\n",
    "    os.makedirs(os.path.join(path))\n",
    "chk_path = path + '/weights.h5'\n",
    "callback = ModelCheckpoint( chk_path, monitor = 'val_accuracy', verbose =1, save_best_only= True,\n",
    "                            save_weights_only =True)\n",
    "earlystop = EarlyStopping(monitor ='val_accuracy', min_delta = 0.001, patience =10)\n",
    "\n",
    "history = model.fit([index_inputs, index_outputs], index_targets,\n",
    "                   batch_size =BATCH_SIZE,\n",
    "                   epochs = EPOCH,\n",
    "                   validation_split= 0.2,\n",
    "                   callbacks = [earlystop, callback])\n",
    "SAVE_FILE_NM = \"weights.h5\"\n",
    "model.load_weights(os.path.join('data_out/seq2seq_ban/weights.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "CHANGE_FILTER = re.compile(FILTERS) # 미리 Complie\n",
    "PAD, PAD_INDEX = \"<PAD>\", 0 # 패딩 토큰\n",
    "STD, STD_INDEX = \"<SOS>\", 1 # 시작 토큰\n",
    "END, END_INDEX = \"<END>\", 2 # 종료 토큰\n",
    "UNK, UNK_INDEX = \"<UNK>\", 3 # 사전에 없음\n",
    "MARKER = [PAD,STD,END,UNK]\n",
    "MAX_SEQUNECE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_processing(value, dictionary):\n",
    "    sequences_input_index = []\n",
    "    sequences_length = []\n",
    "    for sequence in value :\n",
    "        sequence = re.sub(CHANGE_FILTER,\"\",sequence)\n",
    "        sequence_index = []\n",
    "        for word in sequence.split(): # 공백 기준으로 word를 구분\n",
    "            if dictionary.get(word) is not None : # 사전에 있으면\n",
    "                sequence_index.extend([dictionary[word]]) # index 값 쓰고\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        # 길이 제한\n",
    "        if len(sequence_index) > MAX_SEQUNECE:\n",
    "            sequence_index = sequence_index[:MAX_SEQUNECE]\n",
    "\n",
    "        sequences_length.append(len(sequence_index)) # 이 문장의 길이 저장\n",
    "        # Padding 추가\n",
    "        # \"안녕\"  → \"안녕,<PAD>,<PAD>,<PAD>,<PAD>\"\n",
    "        \n",
    "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
    "        \n",
    "        sequences_input_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input('이이')\n",
    "test_index_inputs , _ = enc_processing([query],char2idx)\n",
    "predict_tokens =  model.inference(test_index_inputs)\n",
    "print(' '.join([idx2char['%s'%t] for  t in predict_tokens]))#대답"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
